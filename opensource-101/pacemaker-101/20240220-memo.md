# DAY 1

강사 정보
---
- 최국현
- tang@linux.com

__점심시간:__ 12:00 ~ 13:20+10분까지

__쉬는시간:__ 15분

- [페이스메이커 github 주소](https://github.com/tangt64/training_memos/tree/main/opensource/pacemaker-101)
- [강의 메모 주소](https://github.com/tangt64/training_memos/blob/main/opensource/pacemaker-101/20240220-memo.md)
- [교재PDF 주소](https://github.com/tangt64/training_memos/blob/main/opensource-101/pacemaker-101/101-%EC%98%A4%ED%94%88%EC%86%8C%EC%8A%A4-Pacemaker.pdf)
- [판서 주소](https://wbd.ms/share/v2/aHR0cHM6Ly93aGl0ZWJvYXJkLm1pY3Jvc29mdC5jb20vYXBpL3YxLjAvd2hpdGVib2FyZHMvcmVkZWVtLzIyOTk4OGIzYTNlYTQxZWY5MmU3MzgyZmFkZTc0YjY1X0JCQTcxNzYyLTEyRTAtNDJFMS1CMzI0LTVCMTMxRjQyNEUzRF9lYzczY2I0ZS01OGM1LTRiNTAtYTU3My05ODVhMjA2OTk4NTY=)

강의 진행 시 필요한 소프트웨어는 가상화 소프트웨어 및 리눅스 ISO이미지 입니다. 하이퍼바이저는 윈도우 10/11 Pro기준으로 하이퍼브이 사용합니다.

- [센트OS 9 스트림](https://www.centos.org/download/)
- [로키 9](https://rockylinux.org/news/rocky-linux-9-0-ga-release/)

## 랩 환경

x86_64bit/AMD
---
1. VMware Workstation/Player: 라이선스 문제
2. VirtualBox: vCPU버그 문제 및 네트워크
3. Hyper-V: Windows 10/11 Pro

가상머신
---
- vCPU: 2
- vMEM: 2048MiB, Recommend, 4096MiB
- Node1/2/3: 20GiB
- Node4: 150GiB(targetd server)

만약, 노드를 4대 이상 생성이 어려운 경우는 아래와 같이 생성
- node1/2/3
- node3번이 iSCSI서버가 됨


랩 환경 조건
---
1. 하이퍼브이 기반으로 진행
2. Windows 10/11 Pro 버전(가상화 가속 기능)
3. 리눅스 호스트 기반으로 사용 가능(libvirt기반으로 랩 가능)
4. 버추얼박스, 권장하지 않음(vCPU 문제. 특히 AMD CPU에서 문제가 있음)
5. VMWare Workstation/Player(교육용 라이선스가 불가능)
6. CentOS-9-Stream, Rocky 9(CentOS 8 Stream)


로키 리눅스 사용하지 않는 이유
---
1. pacemaker 및 pcsd의 패키지가 조금 달라짐.
2. 로키 리눅스는 레드햇 라이선스 정책상 SRPM를 그대로 사용이 불가능.
3. fence(stonith)장치가 올바르게 동작하지 않음.(라이브러리 링크가 다름)
4. centos-9-stream기반으로 진행.

"eth0"네트워크 정적으로 변경(static dhcp)
---

```powershell
powershell> New-VMSwitch -SwitchName "Outworld Switch" -SwitchType Internal
powershell> Get-NetAdapter
> ifIndex, <인덱스 번호>
powershell> New-NetIPAddress -IPAddress 192.168.0.1 -PrefixLength 24 -InterfaceIndex 17
powershell> New-NetNat -Name MyNATnetwork -InternalIPInterfaceAddressPrefix 192.168.0.0/24
```


"root"계정으로 로그인 후, 다음처럼 아이피 변경.

```bash
nmcli con sh
# node1, 나머지 노드들은 뒤에 아이피만 변경
nmcli con mod eth0 ipv4.addresses 192.168.0.110/24 ipv4.gateway 192.168.0.1 ipv4.dns 8.8.8.8 ipv4.method manual
nmcli con up eth0

```


노트북/데스크탑(워크스테이션)
---
CPU: 4 cores
MEM: 16GiB, 권장은 32GiB

가상머신: vCPU: 2, vMEM: 4GiB, vDisk 100GiB x 4

**기본과정은 4대면(2,3대) 충분**
node1: cluster node
node2: cluster node
node3: cluster node + (storage + mgmt(vDisk x 4개 더 추가가))
node4: cluster node + storage + mgmt(vDisk x 4개 더 추가가)


# DAY 1


기본 네트워크(Default, NAT)
---
DHCP하이퍼브이에서 제공. 아이피가 하루에 한번 릴리즈가 됨.

정적 네트워크(Static Switch, NAT)
---
DHCP사용하지 않고, 아이피를 고정으로 구성.

내부 네트워크(Internal Switch)
---
외부와 통신이 되지 않고, 내부망으로만 사용.


### 노드 설정

호스트 네임 및 /etc/hosts설정
---

node1/2/3/4에 올바르게 호스트이름 설정이 되었는지 확인.
```bash
hostnamectl
> Static hostname: node1.example.com
hostnamectl set-hostname node1.example.com
cat <<EOF>> /etc/hosts
192.168.90.110 node1.example.com node1
192.168.90.120 node2.example.com node2
192.168.90.130 node3.example.com node3
192.168.90.140 node4.example.com node4
EOF
cat /etc/hosts
```

네트워크 설정 확인
---

모든 노드에서 __"eth1"__ 아이피 주소 확인.
```bash
ip a s eth1
```

SSH 키 생성(관리용)
---
__"node1"__ 번 에서만 실행 및 구성
```bash
ssh-keygen -t rsa -N '' 
dnf install -y sshpass 
cat <<EOF> ~/.ssh/config
StrictHostKeyChecking=no
EOF
for i in {1..4} ; do sshpass -p centos ssh-copy-id root@node${i} ; done
```
```bash
Static Swithc -> Default Switch
[콘솔 열기]
nmcli con up eth0
ip a s eth0
nmtui edit eth0
> DHCP, 나머지 제거
nmcli con up eth0
ip a s eth0
ping 
```

pcs/pacemaker설치
---
```bash
for i in node{1..4} ; do ssh root@$i 'dnf --enablerepo=highavailability -y install pacemaker pcs' ; done
dnf --enablerepo=highavailability -y install pacemaker pcs
```

방화벽 및 서비스 
---
```bash
node1# for i in {1..4} ; do ssh root@node${i} 'firewall-cmd --add-service=high-availability && firewall-cmd --runtime-to-permanent' ; done
node1# for i in {1..4} ; do ssh root@node$i 'echo centos | passwd --stdin hacluster && systemctl enable --now pcsd.service' ; done
```

클러스터 인증 및 생성
---
```bash
node1# pcs host auth -u hacluster -p centos node1.example.com node2.example.com node3.example.com 

node1# pcs cluster setup ha_cluster_lab node1.example.com node2.example.com node3.example.com --start --enable

node1# pcs status
```

테스트! 테스트!
---
```bash
node1# pcs resource create dummy1 ocf:pacemaker:Dummy
node1# pcs resource create vip ipaddr2 ip=192.168.90.250 cidr_netmask=24
node1# pcs resource delete dummy1
node1# pcs resource delete vip
```

# DAY 2

## DRBD 랩


```bash
node1/2# dnf install epel-release -y
node1/2# dnf search drbd
> drbd-pacemaker.x86_64 : Pacemaker resource agent for DRBD
> drbd-rgmanager.x86_64 : Red Hat Cluster Suite agent for DRBD
node1/2# dnf install drbd drbd-bash-completion drbd-pacemaker drbd-utils -y

node1/2# rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
node1/2# yum install https://www.elrepo.org/elrepo-release-9.el9.elrepo.noarch.rpm -y

node1/2# dnf search drbd
node1/2# dnf install kmod-drbd9x -y
node1/2# dnf install kmod-drbd84 -y			## 이게 기본값

node1/2# dnf install kmod-drbd84 drbd drbd-utils -y
node1/2# dnf install kernel-abi-stablelists kernel -y && reboot 

node1/2# modprobe drbd
node1/2# modinfo drbd 
> version: 
node1/2# ls -l /etc/depmod.d
> drbd~
node1/2# ls -l /etc/modprobe
> drbd options

node1/2# systemctl enable --now drbd
node1/2# 

node1/2# dnf install --enablerepo elrepo-kernel kernel-ml -y && reboot
```

1. drbd
오픈소스(공개) 버전의 drbd-core 패키지.
```bash
rpm -ql drbd
rpm -ql drbd-utils
```
2. drbd-pacemaker
3. drbd-bash-completion

### 디버깅

```bash
journalctl -fl -perr -pwarning _COMM=pacemaker-fence

journalctl -fl -perr -pwarning -upacemaker

pcs resource cleanup

pcs stonith history cleanup

cd /var/lib/iscsi/send_targets
> send_targets/
> nodes/

```




```bash
fence_ilo4_ssh -a 192.168.1.100 -x -l hpilofence -p hpilopass -o status 
fence_ilo4_ssh -a 192.168.1.101 -x -l hpilofence -p hpilopass -o status

pcs stonith create server105-cpn_ilo4_fence fence_ilo4_ssh ipaddr="192.168.1.100" login="hpilofence" secure="true" passwd=hpilopass  pcmk_host_list="server105-cpn" delay=10 op monitor interval=60s

pcs stonith create server106-cpn_ilo4_fence fence_ilo4_ssh ipaddr="192.168.1.101" login="hpilofence" secure="true" passwd=hpilopass  pcmk_host_list="server106-cpn" op monitor interval=60s

pcs stonith show --full

```

# DAY 3

drbd
---
- 커널 버전보다는, 릴리즈 시 drbd기능이 활성화 되어 있는지
- 블록 복제가 주요 목적
- 재해복구 목적으로 만들어 짐
- 멀티 노드를 지원은 하나, 접근 단일 노드만 가능
- DRBD 프록시 서비스를 통해서 확장 기능 제공

페이스메이커
---
- libQB, 특정 상황에 대해서 판단.
- "corosync.service", 토템(Totem) 클러스터에서 구성된 노드들 상태 확인(heartbeat)
- "pcs.service", pcs명령어에 대한 서비스 데몬
	+ "crm.service", crm명령어
	+ "crm", "pcs" 둘 다 표준 명령어
- "pacemaker.service", 특정 상황에서 어떻게 동작할지 결정
- resource(monitoring)
	+ 서비스를 모니터링하는 에이전트 프로그램
	+ 특정노드 혹은 클러스터에서 자원을 생성 및 종료
	+ .service(systemd)
	+ shell script(OCF Script, LSB)
- stonith(fence agent)
	+ 노드 차단 조건(node fencing)
	+ 디스크나 혹은 네트워크 혹은 메모리와 같은 장치
	+ NODE_FENCE((DIKS | NETWORK | MEMORY)(TRUE))

## BOOTH

>Typically, multi-site environments are too far apart to support synchronous communication and data replication between the sites. That leads to significant challenges:
>
부스의 주요 목적은 각 다른 사이트(지역)에 구성된 클러스터를 하나로 동기화 하는 기능.

- How do we make sure that a cluster site is up and running?
	+ 얼마나 많은 클러스터가 사이트에서 동작하는가? 
- How do we make sure that resources are only started once?
	+ 리소스 부분. 서비스 시작시 리소스 할당 부분.
- How do we make sure that quorum can be reached between the different sites and a split-brain scenario avoided?
	+ A사이트, B사이트가 각기 다른 쿼럼(정족수)가 도달하였을때 어떻게 S/B를 해결할것인가?
- How do we manage failover between sites?
	+ 각 사이트의 장애 처리?
- How do we deal with high latency in case of resources that need to be stopped
	+ 지연부분(레이턴시)

DR
---
```bash
node1# pcs host auth -uhacluster -pcentos node1.example.com node2.example.com node3.example.com node4.example.com
node1# pcs cluster setup none-dr-nodes node1.example.com node2.example.com --start --enable
node1# pcs cluster setup dr-nodes node3.example.com node4.example.com --start --enable
node1# pcs dr set-recovery-site node3.example.com
node1# pcs dr config
node1# pcs dr status
```

resource
---

1. pcs명령어 입력 순서대로 자원을 구성.
2. 리부팅 이후에는 입력된 순서대로 동작 및 구성이 되지 않음.
3. location를 통해서 리소스 위치를 명시.
4. 동작 순서를 명시하기 위해서는 order 지시자를 사용.


node1
---
```bash
pcs resource create demo-resource ocf:pacemaker:Dummy
pcs resource
```
node2
---
```bash
pcs resource create demo-resource-node2 ocf:pacemaker:Dummy
pcs constraint location demo-resource-node2 prefers node1.example.com=100
pcs resource
pcs constraint config --full
pcs constraint delete location-demo-resource-node2-node1.example.com-100
```

Read Only Account
---

```bash

```

Alert Email
---

```bash

```

ILO Stonith
---

```bash

```



