# DAY 1

- 이름: 최국현
- 메일: tang@linux.com

__쉬는시간:__ 15분

__점심시간:__ 12시부터 01시 20분까지.

메모파일 및 교재
---
https://github.com/tangt64/training_memos/
>/opensource-101/kubernetes-101/20240213-memo.md


화이트보드
---
https://wbd.ms/share/v2/aHR0cHM6Ly93aGl0ZWJvYXJkLm1pY3Jvc29mdC5jb20vYXBpL3YxLjAvd2hpdGVib2FyZHMvcmVkZWVtLzdkNGM4ZTBiMTJhZDRhNTRhYTNkMGRmNWI0N2NkNTY5X0JCQTcxNzYyLTEyRTAtNDJFMS1CMzI0LTVCMTMxRjQyNEUzRF81NzVjMDQ0Mi1hNmU5LTRmNGYtYjY2MS1jNzg0NzcyMGU1ODg=


연습 혹은 실무적 수준으로 랩 구성
---

kubeadm명령어 기반으로 설치
>공식 설치 도구
>강의 진행은 __kubeadm__ 기반으로 진행

minikube/kind는 사용하지 말기.
>AIO/SNO
>master x 1, worker x 2 --> 5EA
>master x 3, L/B Server(VIP), Pacemaker  

자동화 도구
>kubespray(ansible)
>terraform

플랫폼 조합(?)
---
- OpenStack + Kubernetes
- LDAP/FreeIPA(https://www.freeipa.org/)
- Operator(https://sdk.operatorframework.io/)


소프트웨어
---
1. crio(containerd)
2. container-tools(표준 컨테이너 도구)
3. containerd(https://github.com/Mirantis/cri-dockerd) 권장
4. CentOS/Rocky/RHEL/Debian(ubuntu)
5. calico


랩 사양(PoC)
---
- CPU: I5이상에, 8core권장
- MEM: 32GiB이상
- DISK: 1TB, NvME/SSD
- NIC: 1G~10G(baremetal 3G이상)
- OS: Windows 11 Pro + Hyver-V, Rocky Linux 9, CentOS-Stream


## 설치 준비


- [공식 설치 문서](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)
- [OSTREE](https://github.com/ostreedev/ostree)
- [CRIO RUNTIME](https://cri-o.io/)


```bash
dnf search hyper
>hyperv-daemons.x86_64 : Hyper-V daemons suite
>hyperv-daemons-license.noarch : License of the Hyper-V daemons suite
>hyperv-tools.noarch : Tools for Hyper-V guests
>hypervfcopyd.x86_64 : Hyper-V FCOPY daemon
>hypervkvpd.x86_64 : Hyper-V key value pair (KVP) daemon
>hypervvssd.x86_64 : Hyper-V VSS daemon

dnf install -y hyperv-tools hypervfcopyd hypervkvpd hypervvssd hyperv-daemons
reboot

dnf search ostree             ## rhcos, fedora coreos
                    ## ostree, RPM기반


curl -o /etc/yum.repos.d/stable.repo https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/CentOS_9_Stream/devel:kubic:libcontainers:stable.repo

curl -o /etc/yum.repos.d/crio.repo https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/1.24:/1.24.6/CentOS_8/devel:kubic:libcontainers:stable:cri-o:1.24:1.24.6.repo

dnf repolist
dnf search cri-o
dnf install -y cri-o

lsns                  ## 네임스페이스 조회
systemd-cgls              ## cgroup 조회


#
# https://v1-27.docs.kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
# 설치는 버전 1.27로 진행(추후 업그레이드 테스트)
# 

cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.27/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.27/rpm/repodata/repomd.xml.key
exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni
EOF

dnf install -y kubelet kubeadm kubectl --disableexcludes=kubernetes

systemctl enable --now crio
systemctl enable --now kubelet

systemctl is-active crio kubelet
```


쿠버네티스 클러스터 최소 구성 요구사항
---
  One or more machines running a deb/rpm-compatible Linux OS; for example: Ubuntu or CentOS.
  2 GiB or more of RAM per machine--any less leaves little room for your apps.
  At least 2 CPUs on the machine that you use as a control-plane node.
  Full network connectivity among all machines in the cluster. You can use either a public or a private network.

- NIC: 2개
- CPU: 2개
- MEM: 2기가

1. OS를 최신버전으로 업데이트
2. 네트워크 설정

```bash
# /etc/hosts에 A레코드 임시로 명시
vi /etc/hosts
> 192.168.10.1  master1.example.com master1
> 192.168.10.2  node1.example.com node1
> 192.168.10.3  node2.example.com node2


# 이전 네트워크 ifcfg-rh스크립트는 아래 명령어로 처리
nmcli con migrate

# master, node에 동일하게 적용
nmcli con sh
nmcli con mod eth1 ipv4.addresses 192.168.10.1/24         
nmtui edit eth1                          ## TUI기반으로 수정
> IPV4ADDR: 192.168.10.[1..3]/24
> IPV4GW: NONE
> [X] Never use this network for default route  
nmcli con up eth1                        ## nmcli con reload eth1
ip a s eth1

swapon -s
swapoff -a
vi /etc/fstab
systemctl daemon-reload
```

1. 쿠버네티스는 SWAP이 필요가 없다.

network script
---
1. ifcfg-rh
2. ifcfg-suse
3. ifcfg-debian

__systemd__ 를 사용하면서 통합. 

1. NetworkManager(RH)
2. systemd-networkd

```bash
NetworkManager --print-config
cat /etc/NetworkManager/NetworkManager.conf
```

방화벽
---
1. iptables -> nftables변경
2. 고수준 방화벽은 firewalld가 기본(모든 배포판)

```bash
firewall-cmd --get-services | grep kube

for i in kube-api kube-apiserver kube-control-plane kube-control-plane-secure kube-controller-manager kube-controller-manager-secure kube-nodeport-services kube-scheduler kube-scheduler-secure kube-worker kubelet kubelet-readonly kubelet-worker ; do firewall-cmd --add-service=${i} ; done
firewall-cmd --runtime-to-permanent
firewall-cmd --list-all


# 랩에서는 끄고 사용

systemctl disable firewalld
systemctl stop firewalld
```

커널 모듈 및 파라메터(control, worker)
---

```bash
modprobe br_netfilter
lsmod | grep br_netfilter
vi /etc/modules-load.d/10-k8s.conf
> br_netfilter
systemctl daemon-reload

sysctl -w net.ipv4.ip_forward=1
echo net.ipv4.ip_forward=1 > /etc/sysctl.d/10-k8s.conf
sysctl --system
sysctl -a | grep net.ipv4.ip_forward
systemctl daemon-reload
```

설치 테스트
---

```bash
master]# kubeadm init
master]# kubeadm reset --force

kubeadm init phase preflight

## 기본 클러스터 이름은 kubernetes
## 기본 클러스터 도메인은 cluster.local
## 위의 내용을 변경하려면, YAML작성

kubeadm init --apiserver-advertise-address=192.168.10.1 --pod-network-cidr  --service-cidr --service-dns-domain=cluster.local --upload-certs 

## POD네트워크 Pod to Pod의 터널링 네트워크
## SVC네트워크 nftables(iptables)에서 사용하는 SEP(Sevice Endpoint) S/DNAT구현해주는 영역

master]# kubeadm init --apiserver-advertise-address=192.168.10.1 --pod-network-cidr=192.168.0.0/16  --service-cidr=10.1.0.0/16
master]# kubeadm token create --print-join-command

      +-------------+                     +----------+
      |   master1   |                     |  node1   |
      |  [cluster]  |  <---- join ----    | [worker] |
      |  kubernetes |         \           +----------+
      +-------------+          \
                                \
                                 `---> kubeadm join 192.168.10.1:6443 --token bd8j7f.ehaewaeuagnoftpc \
        --discovery-token-ca-cert-hash sha256:2b2af82e2aeb66d94dd49e63e38359091fea5e72244fecfd9fd8a9aff5


KUBECONFIG=/etc/kubernetes/admin.conf kubectl get nodes
> localhost.localdomain
hostnamectl set-hostname master1.example.com
                         node1.example.com
                         node2.example.com        
```

## 컨테이너 및 쿠버네티스 구성 및 구성원

1. runtime filesystem

- /etc/containers/
- /var/lib/containers/storage/overlay
- /run/containers/
- /run/pod/


```bash
lsmod | grep overlay            ## mount
> backingFsBlockDev             ## as block storage

systemctl status kubelet
> kubelet.service - kubelet: The Kubernetes Node Agent
  --------------
  1. 쿠버네티스 설치(이미지 내려받기 및 기본설정)
  2. 컨테이너 기반으로 서비스 구성

crictl pods ls                  ## Pod 컨테이너(인프라)
crictl ps                       ## 컨테이너(애플리케이션)


                          namespace(ipc,mnt,uts,net...)

pod        pause       infra-container
---        -----       ---------------
cgroup     httpd       app-container
namespace

dnf install -y podman 
podman save 6270bb605e12 -o pause.tar
mkdir pause
tar xf pause.tar -C pause
cd pause
tar xf <TAR>

     .---> infra-container
    /
./pause httpd -DFOREGROUND                    ## cgroup+namespace
------- -----                                 ## limit, mount
  POD    APP
    \
     `---> abstration


          kube-proxy/kubelet
                  |
                  |
                CRIO 
        -------------------------
        pod  |  <--->  | container
                  |
                  |
                conmon
     (container monitor utility)
                  |
                  |
                 runc                 ## low level runtime
      (crun, container creator) --- POD
                                    Container

export KUBECONFIG=/etc/kubernetes/admin.conf
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/tigera-operator.yaml

curl https://raw.githubusercontent.com/tangt64/training_memos/main/opensource-101/kubernetes-101/files/calico-quay-crd.yaml -o calico-quay-crd.yaml 
vi calico-quay-crd.yaml
> - blockSize: 26 
>   cidr: 192.168.0.0/16
kubectl apply -f calico-quay-crd.yaml 
kubectl get pods -Aw
setenforce 0
```


https://www.redhat.com/architect/how-kubernetes-creates-runs-containers

https://developers.redhat.com/blog/2019/01/15/podman-managing-containers-pods#podman_pods__what_you_need_to_know

## 명령어 준비


```bash
[root@master1 ~]# kubeadm completion bash > /etc/bash_completion.d/kubeadm.sh
[root@master1 ~]# kubectl completion bash > /etc/bash_completion.d/kubectl.sh
[root@master1 ~]# complete -rp

## 쉘 변수로 사용(admin)
master]# ls -l /etc/kubernetes/
> admin.conf
master]# export KUBECONFIG=/etc/kubernetes/admin.conf

## 파일로 사용(admin)
master]# mkdir -p ~/.kube/
master]# cp /etc/kubernetes/admin.conf ~/.kube/config

master]# kubectl get nodes
```


- kubelet(hosted)
- kube-proxy(containerd, static-pod, network)
- kube-scheduler(containerd, static-pod, pod/container create)
- kube-controller-manager(containerd, static-pod, resource manage )
- kube-apiserver(containerd, static-pod, API 요청)

coredns(domain server)

etcd(kubernetes D/B key-pair+master)
= 마스터 서버는 무조건 3대



# DAY 2

## 제품환경에 대해서 잠깐...

네트워크
---
* API/Mgmt: 1G+
* Storage: 10G+(30G)
* backup: 10G+
* provision: 3G+
* ingress: 3G+
* service: 3G+

Kubernetes-OVN(OVS)

컨트롤(label: control-plane)
---
1. LoadBalancer: Hardware(L4/L7), Software(HAProxy(UDP/TCP), Nginx(TCP), Pacemaker(VIP))
2. control-plane: 3EA(ETCD)

인프라(label: infra, 10G+)
---
1. bind/dnsmasq(named, ingress, metallb)
  - PTR/SRV
2. repository(disconnected installation)
  - kubernetes image
  - calico, ingress, istio
  - buildah, skopeo, podman(workstation)
3. CI/CD
  - Harbor(image)
  - Jenkins/Tekton
  - ArgoCD/Kraken
4. Authentication
  - .X509
  - LDAP
  - OpenAuth(JWT...)
  - OpenStack Keystone
5. Storage
  - GlusterFS
  - Ceph(rook)
  - Moonfs
  - OpenStack Cinder
  - NFS 4.x


컴퓨트(label: compute, 10G+)
---
1. compute x 5EA


## 클러스터 확인


```bash

kubectl get namespaces            ## kubectl get ns
kubectl run test-pod --image=nginx
kubectl get pod                   ## kubectl get pod --namespace=default
kubectl describe pod/test-pod     ## kubectl describe pod test-pod

grep -Ev '^#|^$' /etc/containers/registries.conf
> unqualified-search-registries = ["registry.fedoraproject.org", "registry.access.redhat.com", "docker.io"]
vi /etc/containers/registries.conf
> ["quay.io", "docker.io"]

kubectl run test-apache-3 --image=quay.io/centos7/httpd-24-centos7

kubectl delete pod --all

dnf install -y epel-release
dnf install -y git yamllnit
curl -sS https://webi.sh/vim-ale | sh

```

TOML: INI형태로 사용하였던 설정, TOML

JSON: 개발 데이터 핸들링(reset)

YAML: 인프라 데이터 핸들링(automation)

```yaml
vi deployment.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment  ## Pod이름, crictl pods ls
spec:                     ## Pod사양
  selector:               ## replicaset에서 바로볼 자원
    matchLabels:
      app: nginx
  replicas: 2             ## 복제할 개수(replicaset, rs)
  template:               ## deployment에서 사용하는 설정
    metadata:
      labels:
        app: nginx        ## deployment 레이블
    spec:
      containers:         ## 컨테이너 사양
      - name: nginx       ## 컨테이너 이름, crictl ps 
        image: nginx:1.14.2       ## 컨테이너 이미지
        ports:
        - containerPort: 80       ## 컨테이너 애플리케이션 포트, 포트는 POD에서 관리

kubectl apply -f deployment.yaml
```


```bash
kubectl create deployment test-deployment --dry-run=client --output=yaml --image=nginx --replicas=10 --port=80 --namespace=default > test-deployment.yaml
```


## 네임스페이스

```bash
kubectl create namespace test1
kubectl config set-context --current --namespace=test1
kubectl run test1-pod --image=nginx --namespace=test1

kubectl delete namespace test1

kubectl create namespace test1 --dry-run=client -o=yaml > test1-pod.yaml
kubectl run test1-pod --image=nginx --namespace=test1 --dry-run=client -o=yaml >> test1-pod.yaml 
```

```yaml
---
apiVersion: v1
kind: Namespace
metadata:
  creationTimestamp: null
  name: test1
spec: {}
status: {}
---
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: test1-pod
  name: test1-pod
  namespace: test1
spec:
  containers:
  - image: nginx
    name: test1-pod
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

1.[레드햇 Podman ebook](https://developers.redhat.com/e-books/podman-action)
2.[컨테이너 기본](https://github.com/tangt64/training_memos/tree/main/opensource-101/oci-podman-101)

## 용어 정리

1. low level container(runtime): __crio__, containerd. 저수준 컨테이너는 API기능을 지원하지 않음. 보통 소켓 기반으로 통신. CRI socket지원을 해야 쿠버네티스와 연동이 가능. 컨테이너 구성 자동화.
2. high level container(engine): API기능 지원 및 확장 도구. 예를 들어서 이미지 빌드, 이미지 검색, 이미지 저장, 컨테이너 조작 기능. podman, docker.
3. conmon: lifecycle of Linux containers. Open Container Initiative (OCI) format, 도커 이미지.
4. runc: Open Container Initiative runtime. 실질적으로 컨테이너를 구성.
5. app-container: 일반 컨테이너 이미지
6. infra-container: pause 프로그램이 실행되는 이미지. Pod를 구성 및 생성.


## apply, create

create는 일회성으로 자원을 생성.
- 업데이트가 안됨.
```bash
kubectl create -f ns-basic.yaml
kubectl create -f ns-basic.yaml        ## 이미 생성이 되어있다고 메세지가 출력.
```

apply는 기존의 정보에 applied기록을 남기면서 업데이트가 가능함.
```bash
kubectl apply -f ns-basic.yaml
kubectl apply -f ns-basic.yaml

kubectl edit namespace/ns-basic
```

```bash
kubectl create deployment my-httpd --image=quay.io/centos7/httpd-24-centos7 --port=80 --dry-run=client -o=yaml > my-httpd.yaml
vi my-httpd.yaml
> containers:
>   name: httpd
kubectl apply -f my-httpd.yaml
kubectl get pods -w
vi my-httpd.yaml
> replicas: 1 --> 5
kubectl create -f my-httpd.yaml       ## 동작안됨
kubectl apply -f my-httpd.yaml
kubectl get pod -w
```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: my-httpd
  name: my-hello-nginx
spec:
  replicas: 5
  selector:
    matchLabels:
      app: my-httpd
  template:
    metadata:
      labels:
        app: my-httpd
    spec:
      containers:
      - image: quay.io/redhattraining/hello-world-nginx
        name: httpd
        ports:
        - containerPort: 80
```

```bash
kubectl get pod -A -l=app=my-httpd
# -A: --all-namespaces
# YAML: app: my-httpd
# CLI: app=my-httpd

kubectl create service nodeport test-basic-httpd --tcp=80:80 --node-port=31000 --dry-run=client -o=yaml > test-basic-httpd-svc.yaml

```

연습문제
---


기존에 사용하였던 yaml파일을 사용해서 다음처럼 서비스 구성을 한다.

1. deployment에 이름은 public-httpd라는 이름으로 구성.
2. 생성되는 위치는 basic 네임스페이스에 생성.
3. httpd이미지를 사용해서 yaml파일을 작성.(quay.io/centos7/httpd-24-centos7)
4. 포트번호는 80으로 설정
5. 레이블은 app: test으로 추가
6. 받은 이미지의 replica개수는 1개로 합니다.

```bash
kubectl create deployment public-httpd --namespace=basic --image =quay.io/centos7/httpd-24-centos7 --port=80 --replicas=1 --dry-run=client --output=yaml > public-httpd.yaml
vi pulic-httpd.yaml
kubectl apply -f 
kubectl expose deployment test-basic-httpd --type=NodePort

```

## run

```bash
kubectl run --image=quay.io/centos/centos:stream9 -it bash
> dnf install iproute 
```

```bash
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: runpodt
  name: runpodt
spec:
  containers:
  - command:
    - sleep
  - args:
    - "1000"
    image: quay.io/centos/centos:stream9
    name: runpodt
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

### 연습문제

문제1
---
1. centos이미지로 console-centos라는 이름으로 컨테이너를 실행.
2. 해당 이미지를 동작하기 위해서 sleep프로그램을 10000초로 시작한다
3. httpd이미지(run)를 YAML로 생성 후 apply명령어로 생성한다. 
4. 생성한 자원은 반드시, 'kubectl get pod'명령어로 확인한다. 
5. deployment로 구성이 되면 안됨. pod만 구성해야 됨.

문제2
---
1. nginx이미지를 run명령어로 실행.
2. 이름은 아무거나 상관 없음.

```bash
kubectl run runpod --image=quay.io/centos/centos:stream9 --command --dry-run=client -o=yaml -- sleep 10000 > runpod.yaml

vi runpod.yaml
```





# DAY 3


```bash
git clone https://github.com/kubernetes/examples.git
```


## 디버깅

### journalctl(from kernel)

```bash
journalctl _PID
journalctl _USER -perr -pwarning -pinfo
```


### describe(이벤트)

```bash
kubectl describe <자원> <자원이름>
kubectl describe pod/t1
kubectl describe rs/t1-<ID>
```

### logs(기록)

```bash
kubectl logs t2-765f79554c-n6jrs

cd /var/log/containers/
> cat t2-765f79554c-n6jrs

```

## cp

kubectl cp index.html --> kubelet --> container 


```bash
kubectl run welcome-httpd --image=quay.io/centos7/httpd-24-centos7 --port=80
kubectl expose pod welcome-httpd --port=80 --target-port=8080 --type=NodePort
kubectl get svc
> welcome-httpd   NodePort    10.10.171.201   <none>        80:32342/TCP   4s
curl localhost:32342
32342(node) --> 8080(pod) ---> 80(app)
kubectl exec -it  welcome-httpd -- sh
> ls -l /var/www/html/
> echo "This is welcome-httpd pod" > /var/www/html/index.html
> curl localhost:32342
> exit
vi index.html
<html>
  <head>welcome</head>
<body>
  <h1>Hello World</h1>
</body>
</html>
kubectl cp index.html welcome-httpd:/var/www/html/index.html
curl localhost:32342
```

연습문제
---
- 새로 Pod my-httpd생성 후 index.html파일을 복사한다.
- 해당 index.html파일은 “Hello Apache”라는 문자열을 가지고 있어야 한다.
- 외부에서 올바르게 표시가 되는지 테스트(expose).
- Pod my-httpd를 노드포트로 아무거나 설정 후, 외부에서 브라우저로 접근이 가능해야 함.
- exec를 통해서 index.html파일이 올바르게 구성이 되어 있는지 확인.
- /var/www/html에 올바르게 저장이 되어야 함.

사용할 이미지
---
quay.io/centos7/httpd-24-centos7


## exec

```bash
kubectl exec my-nginx -- ls /usr/share/nginx/html

## 표준 입출력 
# stdinput
# stdout
```

## expose(=service)

port: 컨테이너에서 사용하는 포트. iptables에서 포워드 구성.
```
## APP container
apache, 80/tcp, localhost(loopback)
```

targetport: Pod에서 바인딩 해주는 포트.

```
## Infra container
pause, 80/tcp, localhost(namespace[net])
       ------
       \
        `---> 80/tcp:8080:tcp mapping

```

nodeport: 쿠버네티스에서(모든 노드) 사용하는 서비스 포트
```

apache(80) --- loopback --- pause(8080) --- iptables --- svc(30122)

```

## 잠깐 기본 컨테이너

```bash
dnf install epel-release -y
dnf search podman 
dnf install podman podman-compose podman-docker podman-tui -y
dnf search container-tools
dnf install container-tools -y


podman run --name test-nginx quay.io/centos7/httpd-24-centos7

podman run -d --name test-centos --rm --publish 80 quay.io/centos/centos:stream9 sleep 100000
podman ps
ps -ef | grep sleep
lsns | grep sleep
podman exec -it test-centos /bin/bash
> dnf install iproute procps-ng

```

OS이미지(컨테이너 이미지 파일)은 직접적으로 사용하지 않음. 베이스 이미지 빌드용도로만 사용.

CNI: Container Network Interface 
>/etc/cni/

https://github.com/containers/buildah/blob/main/docs/tutorials/01-intro.md#building-a-container-from-scratch



```yaml
# kubectl create service nodeport test-pod --node-port=31000 --tcp=80:8080 -o=yaml --dry-run=client
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: test-pod
  name: test-pod
spec:
  ports:
  - name: 80-8080
    nodePort: 31000
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: test-pod
  type: NodePort
status:
  loadBalancer: {}
```

```yaml
kubectl expose pod test-pod --type=NodePort --port=80 --target-port=8080  -o=yaml --dry-run=client
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    run: test-pod
  name: test-pod
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    run: test-pod
  type: NodePort
status:
  loadBalancer: {}
```

## label

연습문제
---
```bash
# deployment, run label옵션이 없음.

kubectl create deployment execrise-label --image=nginx --replicas
kubectl scale replicas=3
kubectl label pod, deployment
```

1.  execrise-label를 deployment로 생성
2.  "status=good", "ver=test" 레이블을 추가. 
4.  pod에서 위에 구성된 레이블이 잘 보이는지 확인
5.  3개중 아무거나 레이블 "status=bad"로 하나 설정
6.  최종적으로 selector를 통해서 "status=bad"


## debug

```bash
kubectl debug -it pod/my-httpd --image=quay.io/centos/centos:stream9 --targe
t=my-httpd -- /bin/bash
```

## 랩 이미지
1. quay.io/centos7/httpd-24-centos7
2. quay.io/redhattraining/hello-world-nginx
3. quay.io/centos/centos:stream9

# DAY 4

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: pod-httpd-memcached-mysql
    type: wordpress
  name: pod-httpd-memcached-mysql
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pod-httpd-memcached-mysql
  template:
    metadata:
      labels:
        app: pod-httpd-memcached-mysql
    spec:
      containers:
      - image: quay.io/centos/centos:centos7
        name: httpd
        ports:
        - containerPort: 80
        command: ["sleep", "12000"]
        securityContext:
          capabilities:
            add:
              - all
          allowPrivilegeEscalation: false        
      - image: quay.io/centos/centos:stream9
        name: mysql
        ports:
        - containerPort: 3306
        command: ["sleep", "12000"]  
      - image: quay.io/centos/centos:stream9
        name: memcached
        ports:
        - containerPort: 11211
        command: ["sleep", "12000"]        
```

```bash
httpd]# yum install php php-mysql mod_php httpd -y
httpd]# curl https://wordpress.org/wordpress-5.0.21.tar.gz -o /var/www/html/wordpress.tar.gz
httpd]# cd /var/www/html/
httpd]# tar xzf wordpress.tar.gz -C .
httpd]# mv wordpress/* .
httpd]# httpd -DFOREGROUND

mysql]# yum install mariadb-server
mysql]# mysql_install_db
mysql]# hostname
mysql]# mysqld_safe

httpd]# hostname
> pod-httpd-memcached-mysql-769b677555-2bjqs
httpd]# yum install mariadb
httpd]# mysql -uroot -p -hpod-httpd-memcached-mysql-769b677555-2bjqs
httpd]# 


master]# kubectl expose deployment/pod-httpd-memcached-mysql --type=NodePort -o=yaml --dry-run=client > svc-wordpress.yaml
master]# kubectl apply -f svc-wordpress.yaml
```

## StorageClass/PV/PVC

[NFS CSI 드라이버](https://github.com/kubernetes-csi/csi-driver-nfs/blob/master/docs/install-csi-driver-v4.6.0.md)

```bash
curl -skSL https://raw.githubusercontent.com/kubernetes-csi/csi-driver-nfs/v4.6.0/deploy/install-driver.sh | bash -s v4.6.0 --

dnf install nfs-utils -y 
systemctl enable --now nfs-server
vi /etc/exports
> /nfs/    *(rw,sync,no_root_squash)
mkdir -p /nfs/
exportfs -avrs
showmount -e master1.example.com

kubectl get csidrivers.storage.k8s.io 
kubectl get pv              ## persistentVolume
kubectl get pvc             ## persistentVolumeClaim
kubectl get sc              ## storageclass

kubectl create namespace sc-ns
kubectl config set-context --current --namespace=sc-ns

vi nfs-provisioner.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: default
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner:nfs.csi.k8s.io
parameters:
  server: master1.example.com/
  share: /nfs

kubectl apply -f nfs-provisioner.yaml
```

```yaml
cat <<EOF> storageclass-sa.yaml
kind: ServiceAccount
apiVersion: v1
metadata:
  name: nfs-pod-provisioner-sa
EOF
kubectl get sa

```


```yaml
cat <<EOF> storageclass-clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-provisioner-clusterRole
rules:
  - apiGroups: [""] # rules on persistentvolumes
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]
EOF
```


```yaml
cat <<EOF> storageclass-clusterrolebind.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-provisioner-rolebinding
subjects:
  - kind: ServiceAccount
    name: nfs-pod-provisioner-sa
    namespace: default
roleRef:
  kind: ClusterRole
  name: nfs-provisioner-clusterRole
  apiGroup: rbac.authorization.k8s.io
EOF

```

```yaml
cat <<EOF> storageclass-role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-pod-provisioner-otherRoles
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
EOF
```

```yaml
cat <<EOF> storageclass-rolebinding.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-pod-provisioner-otherRoles
subjects:
  - kind: ServiceAccount
    name: nfs-pod-provisioner-sa
    namespace: default
roleRef:
    kind: Role
    name: nfs-pod-provisioner-otherRoles
    apiGroup: rbac.authorization.k8s.io
EOF
```
```yaml
cat <<EOF> storageclass-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-nfs-dynamic
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
  storageClassName: "nfs-csi"
EOF
```

```bash
node1]# dnf install nfs-utils -y
node1]# showmount -e master1.example.com
```

```yaml
cat <<EOF> nfs-csi-pod.yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  name: httpd-csi
spec:
  selector:
    matchLabels:
      app: httpd-csi
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: httpd-csi
    spec:
      serviceAccountName: nfs-pod-provisioner-sa
      containers:
        - name: pvc-httpd
          image: quay.io/centos7/httpd-24-centos7
          volumeMounts:
            - name: csi-nfs
              mountPath: /var/www/html/
          ports:
          - containerPort: 80
      volumes:
       - name: csi-nfs
         nfs:
           server: master1.example.com
           path: /nfs
EOF

```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-nfs
spec:
  replicas: 1
  selector:
    matchLabels:
      name: deployment-nfs
  template:
    metadata:
      name: deployment-nfs
      labels:
        name: deployment-nfs
    spec:
      nodeSelector:
        "kubernetes.io/os": linux
      containers:
        - name: deployment-nfs
          image: mcr.microsoft.com/oss/nginx/nginx:1.19.5
          command:
            - "/bin/bash"
            - "-c"
            - set -euo pipefail; while true; do echo $(hostname) $(date) >> /mnt/nfs/outfile; sleep 1; done
          volumeMounts:
            - name: nfs
              mountPath: "/mnt/nfs"
              readOnly: false
      volumes:
        - name: nfs
          persistentVolumeClaim:
            claimName: pvc-deployment-nfs
```

```yaml
cat <<EOF> manual-pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
  labels:
    type: nfs
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteMany
nfs:
    server: master.example.com
    path: "/nfs/manual-pv"
EOF

```

```yaml
cat <<EOF> pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-pvc
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
EOF
```