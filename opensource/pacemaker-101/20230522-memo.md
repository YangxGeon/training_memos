# DAY 1


## 랩 구성

```bash
              +---------------------+
              | switch for external | - Default Switch(hyper-v)
              +---------------------+ - NAT(VMware)                    
                         |
                         |
                        [external]
                        NIC(vmnet0)
                         | (eth0,hyper-v)
                         |
                    +--------+
                    |   VM   |  
                    +--------+
                         |
                         | (eth1,hyper-v)
                        NIC(vmnet1)
                        [internal]   VM to VM
                         |
                         |
              +---------------------+
              | switch for internal | - internal(hyper-v)
              +---------------------+ - host(VMware)                     
```

[윈도우 터미널 설치](https://learn.microsoft.com/ko-kr/windows/terminal/install)

[하이퍼브이 설치 메뉴얼](https://learn.microsoft.com/ko-kr/virtualization/hyper-v-on-windows/quick-start/enable-hyper-v)

- 네트워크
    + internal: 내부 네트워크(하이퍼브이 및 다른 하이퍼바이저)
    + external: 외부 네트워크(하이퍼브이는 'default')
    + storage: 저장소 네트워크(없어도 상관은 없음)
- 가상머신
    + 최소 가상머신 3대로 구성
    + 가상머신의 이름은 node1/2/3으로 구성
    + vMEM: 4096MB(최소2048)
    + vCPU: 2개 권장
    + vDISK: Thin, 100GB/Fixed, 30GB
- 리눅스 이미지
    + CentOS-9-Stream(강의에서는 이 이미지 기반으로 진행)
    + Rocky-9(RHEL Clone)
    + Oracle-Linux-9
    + OpenSuSE
    + Ubuntu/Debian
    

### 네트워크 설정


두 번째 네트워크 카드의 주소를 고정 아이피로 설정.

- nmcli
- nmtui

```bash
nmcli con show
NAME  UUID                                  TYPE      DEVICE
eth0  0c510583-c37d-398f-835d-2c593f009085  ethernet  eth0
eth1  552198fe-f5a3-31d7-a945-2394b577c620  ethernet  eth1
lo    5426e314-719c-408d-812b-a1be8b0b6146  loopback  lo

nmcli dev

nmcli con mod
          add    ifname ens160

nmcli con down eth1
nmcli con up eth1
nmcli con sh eth1 | grep ipv4. | less

nmcli del <UUID>                            ## 필요없는 프로파일 제거

```
"프로파일"이름이 장치이름으로 안되어 있는 경우 'add'로 프로파일 추가 생성.

아이피 주소는 다음과 같이 구성

| 노드 이름 | 아이피 주소 | 게이트웨이|
|-----------|-------------|-----------|
| node1     | 192.168.90.110/24 | N/A |
| node2     | 192.168.90.120/24 | N/A |
| node3     | 192.168.90.130/24 | N/A |
| node4     | 192.168.90.140/24 | N/A | 

**혹시나 하여, eth1를 기본 라우트 아이피로 사용하지 않음.

하이퍼브이
---

```bash

## node1 
nmcli con mod eth1 ipv4.addresses 192.168.90.110/24 ipv4.never-default yes ipv4.method manual autoconnect yes
nmcli con up eth1

## node2
nmcli con mod eth1 ipv4.addresses 192.168.90.120/24 ipv4.never-default yes ipv4.method manual autoconnect yes
nmcli con up eth1

## node3
nmcli con mod eth1 ipv4.addresses 192.168.90.130/24 ipv4.never-default yes ipv4.method manual autoconnect yes
nmcli con up eth1

## node4 
nmcli con mod eth1 ipv4.addresses 192.168.90.140/24 ipv4.never-default yes ipv4.method manual autoconnect yes
nmcli con up eth1
```


VMware, VirtualBox, Libvirt
---

장치이름이 eno1, ens1...이름이 다르게 구성이 됨.

- 프로파일 이름은 보통 장치 이름과 동일하게 구성
- 변경해야 될 부분은 'con-name', 'ifname', 'type'
- con-name, connection name의 줄임
- ifname, interface card name의 줄임

```bash
## node1
nmcli con add con-name ens1 ipv4.addresses 192.168.90.110/24 ipv4.never-default yes ipv4.method manual autoconnect yes type ethernet ifname ens1 
nmcli con up ens1

## node2
nmcli con add con-name ens1 ipv4.addresses 192.168.90.120/24 ipv4.never-default yes ipv4.method manual autoconnect yes type ethernet ifname ens1 
nmcli con up ens1

## node3
nmcli con add con-name ens1 ipv4.addresses 192.168.90.130/24 ipv4.never-default yes ipv4.method manual autoconnect yes type ethernet ifname ens1 
nmcli con up ens1

## node4
nmcli con add con-name ens1 ipv4.addresses 192.168.90.140/24 ipv4.never-default yes ipv4.method manual autoconnect yes type ethernet ifname ens1 
nmcli con up ens1

```

## 페이스 메이커 소개


페이스메이커는 스스로 H/A기능을 제공하지 않음. H/A를 구성하기 위해서 사용하는 자동화 도구. 


[에이전트 개발자 가이드](https://github.com/ClusterLabs/resource-agents/blob/main/doc/dev-guides/ra-dev-guide.asc)


### 9 이론

90~99%: 애플리케이션 영역, 90% H/A애플리케이션이 구성이 되었을 때. 

- 99%: 일단위로 전환
- 99.9%: 시간단위로 전환
- 99.99995: 10초 이내

보통 보고서에 많이 적는 문구 "99.999%" 복구율 보장(2~5분 사이). 페이스메이커는 99.999% ~ 99.9999%를 지원.

99.99%를 지원하는 경우, 페이스메이커가 멀티 사이트(Multi-Site)로 구성이 되어 있는 경우.

본레 페이스메이커는 D/R를 지원하지 않음. RHEL 8버전 이후부터는 D/R 및 BOOTH기능이 추가가 됨.

D/R, H/A 리소스를 그닥 효율적으로 사용하지 않다. 
- A/A가 아닌 경우 
- 컨테이너 시스템에서 D/R, H/A가 필요하지는 않음


## 설치 준비

```bash

              |     pacemaker    |   <-- systemctl start pcsd
              +------------------+
                \
                 `---> crm_*
```





```bash
dnf grouplist 

## 확장 패키지는 앞으로는 모듈로 제공(app-stream)
dnf module list

## /etc/yum.repos.d/centos-addons.repo에서 사용하지 않도록 설정이 되어 있음. 
ls -l /etc/yum.repos.d/centos-addons.repo
vi /etc/yum.repos.d/centos-addons.repo
enabled=0 --> 1

## 옵션으로 저장소 활성화
dnf --enablerepo=highavailability search pacemaker pcs
dnf --enablerepo=highavailability install pacemaker pcs -y     ## 모든 노드에 설치

systemctl is-active pcsd.service
systemctl enable --now pcsd.service

locale 
> ko_KR.utf8 --> en_US.utf8
             --> C
localectl set-locale en_US.utf8   ## 1
export LC_ALL=en_US.utf8          ## 2

kill -9 1754
dnf install glibc-langpack-en -y

localectl set-locale C
export LC_ALL=C


hostnamectl                                                ## 호스트 이름 확인
cat <<EOF>> /etc/hosts                                     ## node3번에서 작업중 
192.168.90.110 node1.example.com node1                     ## sda,b,c,d(iscsi)
192.168.90.120 node2.example.com node2                     ## sda,b,c,d(iscsi)
192.168.90.130 node3.example.com node3                     ## 블록장치를 추가 b,c,d + e,f,g(iscsi)
192.168.90.140 node4.example.com node4 storage cli utility ## 파일기반으로 블록장치 구성
EOF

sshpass -pcentos scp /etc/hosts node1:/etc/hosts

ping -c2 node{1..3}

ssh-keygen -t rsa -N'' ~/.ssh/id_rsa                      ## ssh 비공개/공개키 생성
dnf install sshpass -y                                    ## sshpass 패스워드 입력 대신  
cat <<EOF> ~/.ssh/config                                  ## fingerprint 무시
StrictHostKeyChecking=no
EOF
sshpass -pcentos ssh-copy-id root@node{1..3}              ## node1~3번까지 공개키 전달
sshpass -pcentos scp /etc/hosts node{1..3}:/etc/hosts

## 루트 로그인이 안되시는 경우
## 현재 기본값 PermitRootLogin prohibit-password
## RHEL9이나 혹은 rocky 9, Centos-9-Stream에서 변경사항

/etc/ssh/sshd_config.d/01-rootallow.conf
PermitRootLogin yes                                       ## /etc/ssh/sshd_config의 값 오버라이드 

## SSH passphase키 질문 부분
vi /etc/ssh/sshd_config                                   ## 앞으로는 절대로 직접 수정하지 말것
cat <<EOF> /etc/ssh/sshd_config.d/02-keyallow.conf
PubkeyAuthentication yes
EOF

systemctl restart sshd

## node 1~3번까지 확인

sshpass -pcentos ssh node1 "hostname && grep hacluster /etc/passwd"
sshpass -pcentos ssh node2 "hostname && grep hacluster /etc/passwd"
sshpass -pcentos ssh node3 "hostname && grep hacluster /etc/passwd"


grep hacluster /etc/passwd                            ## pacemaker hacluster 사용자, PCS 루트 사용자

## node1~3번까지 패스워드 설정

echo cluster | passwd --stdin hacluster
grep hacluster /etc/passwd
hacluster:x:189:189:cluster user:/home/hacluster:/sbin/nologin               ## pcs명령어 접근 및 실행시 사용

systemctl enable --now pcsd.service                                          ## node1, node2, node3
systemctl stop firewalld                                                     ## 방화벽 중지
pcs host auth -u hacluster -p cluster node1.example.com node2.example.com  node3.example.com  ## node3
> node1.example.com: Authorized
> node2.example.com: Authorized
> node3.example.com: Authorized
#
# /var/lib/pacemaker 각 노드끼리 토큰 인증
#

pcs status
pcs cluster status
pcs cluster setup ha_cluster_lab node1.example.com node2.example.com node3.example.com
                  -------------  ----------------- -----------------
                  클러스터 이름      노드 이름         노드 이름       .......
--> 만약에 이전 내용이 존재하면 제거를 시도
--> pcs == pcsd == pacemaker
--> corosync == quorum
pcs cluster start --all
pcs cluster enable --all 

pcs cluster status
pcs status

## pcs ---> pcsd.service


ls -laR /etc/corosync
ls -laR /etc/pacemaker
ls -laR /var/lib/corosync
ls -laR /var/lib/pacemaker
``` 



페이스메이커 설치시 가급적이면, terraform, ansible, puppet를 통해서 자동으로 설치 및 구성을 권장.
- 레드햇 계열은 앤서블 기반으로 설치



### 컨테이너 잠깐...

docker ---> podman 
                    ---> kubernetes ---> pod ---> svc


```bash
dnf install epel-release -y
dnf install podman podman-compose podman-docker podman -y
dnf install git -y

git clone https://github.com/docker/awesome-compose/
> tree/master/apache-php
podman-compose up -d
podman container ls
podman kube generate apache-php_web_1 --service -f apache.yaml 

kubectl apply -f apache.yaml

```

### 레드햇 클론의 고뇌?

```bash
                  ---- 간주 넘기기 ----> 
 과도기                  안정버전((레거시))   컨테이너+가상화+OS REV
                  |
 RHEL 7           |       RHEL 8            |        RHEL 9             |   RHEL  X(10)
                  |
+ systemd           + systemd                + systemd(완성형)
+ NetworkManager    + xfs rev.2              + xfs rev(d)
+ teamd             + kernel --> 4.x         + kernel 5.x 
+ OCI               * container              + container 성능
+ xfs rev.1                                  + network, block 성능 
                                             + iptables -> nftable(firewalld)
                                             + NetworkManager(d)
                                             + LVM2 --> Stratis, Vdo(R)
                                             
```

RHEL 9에서 많이 달라진 부분
---

1. 커널: 컨테이너+네트워크+프로세스
2. 블록장치: xfs, SGI 1:1, 성능위주 ---> 편의성+
3. 네트워크 TCP Stack, 다중처리에 초점(컨테이너 및 가상머신), eBPF(현 리눅스, cBPF)
4. 스토리지: Stratis Pool, Vdo 

__컨테이너 사용하시는 경우__

가급적이면, SELinux나 혹은 AppAmmor [Selinux vs AppArmor](https://phoenixnap.com/kb/apparmor-vs-selinux)

리눅스 파운데이션   --> AppArmor      ## 커뮤니티...장점이 상대적으로 사용하기 편하다.
엔터프라이즈 리눅스 --> SELinux       ## NIST 표준임


아직 까지는 LVM2가 리눅스에서 Pool기준임. 
- btrfs, 네이티브 스토리지 풀링, zfs, ufs, jfs와 동일한 기능
- Stratis(LVM2를 재구성), xfs전용


# DAY 2


## 정리

프로비저닝
---
- satellite == foreman(dhcp,tftp,pxe)
- dhcp, tftp, pxe


디폴로이먼트
---
- ansible(레드햇 권장)
- terraform
- SH script(현재는 권장하지 않음)
- 내부 NTP서버도 필요(방화벽에서 특정 서버만 허용)

1. 페이스메이커 설치는 폐쇠망에서 설치
2. RPM에 대한 저장소 미러링이 필요함
3. 자동화 도구(예,앤서블) 기반으로 설치 및 설정을 자동화
4. NTP서버
5. Corosync(시간에 매우 민감)



### iscsi 서버 구성

1. 실제로 물리적인 스토리지 서버 사용이 불가능
2. 자원이 넉넉하지 않는 관계로, 파일 기반의 블록장치를 만들어서 배포
3. target기반으로 각각 서버에서 /dev/sdb,c,d를 배포

```bash
dnf install targetcli -y                                ## node4번에만 설치
systemctl enable --now target
firewall-cmd --add-service=iscsi-target                 ## 방화벽 사용을 안하시면 해당 부분 무시
dnf install iscsi-initiator-utils -y                    ## 모든노드에서 설치가 되어야 됨
```

```bash

## fileio 파일 기반으로 가상의 디스크를 생성
## 여기서 사용하는 이름은 분류를 위한 파일 네이밍

setenforce 0
getenforce
mkdir -p /var/lib/iscsi_disks/
targetcli backstores/fileio create iscsi /var/lib/iscsi_disks/iscsi_disk.img 2G
targetcli backstores/fileio create nfs /var/lib/iscsi_disks/nfs_disk.img 2G
targetcli backstores/fileio create gfs2 /var/lib/iscsi_disks/gfs2_disk.img 2G
ls -l /var/lib/iscsi_disks/
file /var/lib/iscsi_disks/iscsi_disk.img
> /var/lib/iscsi_disks/iscsi_disk.img: data
targetcli ls

targetcli iscsi/ create iqn.2023-02.com.example:blocks  
targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/luns/ create /backstores/fileio/iscsi/
targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/luns/ create /backstores/fileio/nfs/
targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/luns/ create /backstores/fileio/gfs2/
targetcli ls


## IQN ACL 구성 

targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/acls/ create iqn.2023-02.com.example:node1.init
targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/acls/ create iqn.2023-02.com.example:node2.init
targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/acls/ create iqn.2023-02.com.example:node3.init
targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/acls/ create iqn.2023-02.com.example:node4.init
targetcli ls

firewall-cmd --add-service=iscsi-target
firewall-cmd --runtime-to-permanent

systemctl stop firewalld                                      ## 방화벽 필요 없으면

targetcli saveconfig

## 각 노드에 다음과 같이 수정

vi /etc/iscsi/initiatorname.iscsi
InitiatorName=iqn.2023-02.com.example:nodeX.init

cat <<EOF>> /etc/iscsi/initiatorname.iscsi
InitiatorName=iqn.2023-02.com.example:nodeX.init
EOF

## 각 노드에서 아래와 같이 명령어 실행
systemctl restart iscsi iscsid
iscsiadm -m discovery -t sendtargets -p 192.168.90.140 
> 192.168.90.140:3260,1 iqn.2023-02.com.example:blocks              ## 올바르게 iscsi 질의 완료

iscsiadm -m node --login                                            ## 각 노드에서 실행
lsblk 
> sdb           8:16   0     2G  0 disk
> sdc           8:32   0     2G  0 disk
> sdd           8:48   0     2G  0 disk
```

## 클러스터 상태 확인

RHEL 7 이후에 pacemaker는 /etc/에 있는 설정 파일을 수동으로 편집을 하는 경우가 없음. RHEL 8/9부터는 수동으로 편집을 권장하지 않음. 

일반적인 장애들은, SELinux나 혹은 /var/에서 디렉터리 손실이나 혹은 설정(퍼미션 같은) 문제로 발생.

```bash
pcs status                                  ## 페이스메이커 설정 확인
pcs cluster status                          ## pcsd, corosync 서비스 상태
pcs status corosync                         ## corosync의 구성원 현황

systemctl status pcsd pacemaker corosync
          is-active

# man corosync                                ## /etc/corosync/

ls -l /etc/corosync/ 
> corosync.conf                             ## corosync 설정파일. pcs명령어에 따라서 내용이 변경
ls -l /etc/pacemaker/
> authkey                                   ## pcs auth로 통해서 인증 받은 정보가 있음. 페이스메이커 관리자 계정
ls -l /etc/ha.d/                            ## OCF 스크립트 펑션
ls -l /var/pacemaker/ 
                     cib/cib.xml            ## corosync에서 사용하는 노드 동기화 정보           
ls -l /var/pcsd/
                known-hosts                 ## ss -antp | grep 2224
ls -l /var/corosync/
```

로그 부분

```bash
/var/log/pcsd                               ## pcsd 로그
         pacemaker                          ## pacemaker 로그
         cluster                            ## corosync 로그


journalctl                                  ## /run/log/journal

## 영구적으로 기록을 남기게 하려면 아래와 같이 (쉬운 방법)
cp -a /run/log/journal/   /var/log/journal
systemctl daemon-reload
systemctl restart systemd-journald

## pcsd pacemaker corosync

journalctl -u pcsd                                        ## /var/log/pcsd

journalctl -u pacemaker -fl -p err -p warning -p info     ## /var/log/pacemaker/ 

journalctl -u corosync                                    ## /var/log/cluster/
 
journalctl -u pacemaker -p err -p warning -o cat          ## -o, --output 형태로 파일 출력
```


연습문제
---
**잠깐 문제(5분)**

journalctl명령어를 사용해서 pcsd, pacemaker, corosync에서 발생한 오류 및 경고 메세지 확인.
발생한 오류 메세지를 텍스트 형태로 각각 서비스 이름으로 .txt확장로 저장.